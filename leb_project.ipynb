{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Documents\\Deep_learning\\leb_project\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "cwd = os.getcwd()\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPool2D,Dense,Dropout,Softmax,Input,Flatten\n",
    "from keras.optimizers import Adam,RMSprop,SGD\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score,roc_curve,accuracy_score,recall_score\n",
    "from keras.metrics import categorical_accuracy\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import set_random_seed\n",
    "os.environ['PYTHONHASHSEED'] = \"0\"\n",
    "np.random.seed(1)\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4879648024625938754\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1523639910\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 12396329270807074523\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 760, pci bus id: 0000:01:00.0, compute capability: 3.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend.tensorflow_backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation=\"relu\", padding=\"same\",\n",
    "                 input_shape=(64,64,1)))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation=\"relu\", padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), activation=\"relu\", padding=\"same\"))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), activation=\"relu\", padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024,activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(rate=0.4))\n",
    "model.add(Dense(2, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=0.001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "gen = ImageDataGenerator()\n",
    "train_batches = gen.flow_from_directory(cwd + \"/chest_xray/train\",model.input_shape[1:3],color_mode=\"grayscale\",shuffle=True,seed=1,\n",
    "                                        batch_size=16)\n",
    "valid_batches = gen.flow_from_directory(cwd + \"/chest_xray/val\", model.input_shape[1:3],color_mode=\"grayscale\", shuffle=True,seed=1,\n",
    "                                        batch_size=16)\n",
    "test_batches = gen.flow_from_directory(cwd + \"/chest_xray/test\", model.input_shape[1:3], shuffle=False,\n",
    "                                       color_mode=\"grayscale\", batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "326/326 [==============================] - 50s 152ms/step - loss: 0.2994 - acc: 0.9080 - val_loss: 6.7607 - val_acc: 0.5000\n",
      "Epoch 2/3\n",
      "326/326 [==============================] - 48s 147ms/step - loss: 0.1517 - acc: 0.9450 - val_loss: 5.5757 - val_acc: 0.5625\n",
      "Epoch 3/3\n",
      "326/326 [==============================] - 48s 147ms/step - loss: 0.1342 - acc: 0.9544 - val_loss: 4.4467 - val_acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "with K.tf.device('/gpu:0'):\n",
    "    model.fit_generator(train_batches,validation_data=valid_batches,epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "326/326 [==============================] - 49s 149ms/step - loss: 0.0694 - acc: 0.9722 - val_loss: 1.3681 - val_acc: 0.6250\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0524 - acc: 0.9810 - val_loss: 0.0526 - val_acc: 1.0000\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 47s 146ms/step - loss: 0.0505 - acc: 0.9806 - val_loss: 1.0031 - val_acc: 0.5625\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 48s 146ms/step - loss: 0.0510 - acc: 0.9816 - val_loss: 0.8431 - val_acc: 0.6250\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0440 - acc: 0.9852 - val_loss: 0.1560 - val_acc: 0.9375\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 47s 146ms/step - loss: 0.0400 - acc: 0.9835 - val_loss: 0.9677 - val_acc: 0.6250\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0413 - acc: 0.9850 - val_loss: 0.5156 - val_acc: 0.6875\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0337 - acc: 0.9883 - val_loss: 1.3233 - val_acc: 0.5625\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0387 - acc: 0.9864 - val_loss: 2.1042 - val_acc: 0.5625\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0369 - acc: 0.9872 - val_loss: 0.5591 - val_acc: 0.7500\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 47s 146ms/step - loss: 0.0315 - acc: 0.9879 - val_loss: 2.3993 - val_acc: 0.5625\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0326 - acc: 0.9889 - val_loss: 0.0247 - val_acc: 1.0000\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0292 - acc: 0.9896 - val_loss: 0.0852 - val_acc: 1.0000\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0296 - acc: 0.9898 - val_loss: 0.3388 - val_acc: 0.8750\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0283 - acc: 0.9895 - val_loss: 0.0352 - val_acc: 1.0000\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 48s 146ms/step - loss: 0.0239 - acc: 0.9916 - val_loss: 0.0500 - val_acc: 1.0000\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 48s 147ms/step - loss: 0.0229 - acc: 0.9921 - val_loss: 0.0703 - val_acc: 1.0000\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0259 - acc: 0.9912 - val_loss: 0.6076 - val_acc: 0.6250\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0233 - acc: 0.9918 - val_loss: 1.4612 - val_acc: 0.6875\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0233 - acc: 0.9929 - val_loss: 0.0346 - val_acc: 1.0000\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0175 - acc: 0.9939 - val_loss: 0.3661 - val_acc: 0.7500\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0207 - acc: 0.9935 - val_loss: 1.6932 - val_acc: 0.6250\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0222 - acc: 0.9921 - val_loss: 0.2577 - val_acc: 0.8750\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0136 - acc: 0.9960 - val_loss: 1.1290 - val_acc: 0.6875\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0149 - acc: 0.9954 - val_loss: 0.0159 - val_acc: 1.0000\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0166 - acc: 0.9948 - val_loss: 1.3739 - val_acc: 0.6875\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0100 - acc: 0.9962 - val_loss: 0.2397 - val_acc: 0.9375\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0172 - acc: 0.9942 - val_loss: 1.7987 - val_acc: 0.5625\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0115 - acc: 0.9960 - val_loss: 0.1938 - val_acc: 0.8750\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 48s 146ms/step - loss: 0.0124 - acc: 0.9956 - val_loss: 0.2245 - val_acc: 0.8125\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0122 - acc: 0.9960 - val_loss: 0.1787 - val_acc: 0.9375\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0127 - acc: 0.9960 - val_loss: 0.0170 - val_acc: 1.0000\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0126 - acc: 0.9958 - val_loss: 0.4542 - val_acc: 0.7500\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0168 - acc: 0.9948 - val_loss: 0.0512 - val_acc: 1.0000\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0126 - acc: 0.9956 - val_loss: 0.2394 - val_acc: 0.9375\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0078 - acc: 0.9973 - val_loss: 0.0163 - val_acc: 1.0000\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 47s 146ms/step - loss: 0.0098 - acc: 0.9964 - val_loss: 0.1341 - val_acc: 0.9375\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0104 - acc: 0.9964 - val_loss: 0.2504 - val_acc: 0.8750\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0142 - acc: 0.9960 - val_loss: 0.1725 - val_acc: 0.8750\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0101 - acc: 0.9960 - val_loss: 1.0140 - val_acc: 0.6250\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 47s 146ms/step - loss: 0.0177 - acc: 0.9942 - val_loss: 0.3143 - val_acc: 0.8125\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0125 - acc: 0.9952 - val_loss: 0.0161 - val_acc: 1.0000\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0101 - acc: 0.9967 - val_loss: 0.0105 - val_acc: 1.0000\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0076 - acc: 0.9971 - val_loss: 0.1990 - val_acc: 0.9375\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 48s 147ms/step - loss: 0.0096 - acc: 0.9967 - val_loss: 0.6323 - val_acc: 0.8125\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0099 - acc: 0.9967 - val_loss: 0.8753 - val_acc: 0.7500\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0078 - acc: 0.9971 - val_loss: 0.9833 - val_acc: 0.6875\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0079 - acc: 0.9973 - val_loss: 0.0349 - val_acc: 1.0000\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0081 - acc: 0.9977 - val_loss: 0.6299 - val_acc: 0.6875\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 47s 146ms/step - loss: 0.0092 - acc: 0.9977 - val_loss: 0.0196 - val_acc: 1.0000\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0057 - acc: 0.9983 - val_loss: 0.0159 - val_acc: 1.0000\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0072 - acc: 0.9985 - val_loss: 0.2106 - val_acc: 0.8750\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 48s 146ms/step - loss: 0.0059 - acc: 0.9981 - val_loss: 0.0233 - val_acc: 1.0000\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0069 - acc: 0.9973 - val_loss: 0.0392 - val_acc: 1.0000\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0092 - acc: 0.9965 - val_loss: 0.2298 - val_acc: 0.9375\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0091 - acc: 0.9969 - val_loss: 0.1027 - val_acc: 0.9375\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0128 - acc: 0.9962 - val_loss: 0.0119 - val_acc: 1.0000\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0086 - acc: 0.9977 - val_loss: 0.0505 - val_acc: 1.0000\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0052 - acc: 0.9985 - val_loss: 0.9618 - val_acc: 0.8125\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0043 - acc: 0.9992 - val_loss: 0.0400 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0082 - acc: 0.9973 - val_loss: 2.4050 - val_acc: 0.7500\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0084 - acc: 0.9975 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0102 - acc: 0.9964 - val_loss: 0.8824 - val_acc: 0.7500\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0049 - acc: 0.9987 - val_loss: 0.4194 - val_acc: 0.7500\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0075 - acc: 0.9965 - val_loss: 0.3901 - val_acc: 0.8125\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0092 - acc: 0.9967 - val_loss: 0.0279 - val_acc: 1.0000\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0066 - acc: 0.9981 - val_loss: 1.1568 - val_acc: 0.7500\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0100 - acc: 0.9965 - val_loss: 0.8187 - val_acc: 0.7500\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0064 - acc: 0.9979 - val_loss: 0.3268 - val_acc: 0.8750\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0046 - acc: 0.9983 - val_loss: 0.8125 - val_acc: 0.7500\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0046 - acc: 0.9985 - val_loss: 0.0674 - val_acc: 1.0000\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0087 - acc: 0.9967 - val_loss: 1.4917 - val_acc: 0.7500\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0088 - acc: 0.9971 - val_loss: 0.4600 - val_acc: 0.8750\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0089 - acc: 0.9969 - val_loss: 0.5393 - val_acc: 0.7500\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0047 - acc: 0.9985 - val_loss: 0.4913 - val_acc: 0.8125\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0047 - acc: 0.9988 - val_loss: 0.3845 - val_acc: 0.8125\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0026 - acc: 0.9994 - val_loss: 0.2581 - val_acc: 0.8125\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0050 - acc: 0.9981 - val_loss: 0.7824 - val_acc: 0.6875\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0050 - acc: 0.9983 - val_loss: 0.2158 - val_acc: 0.8750\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0037 - acc: 0.9988 - val_loss: 4.8661 - val_acc: 0.5625\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0046 - acc: 0.9981 - val_loss: 0.0350 - val_acc: 1.0000\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0049 - acc: 0.9985 - val_loss: 0.5688 - val_acc: 0.8750\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0088 - acc: 0.9971 - val_loss: 0.3296 - val_acc: 0.8750\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0059 - acc: 0.9981 - val_loss: 0.4946 - val_acc: 0.8125\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0040 - acc: 0.9987 - val_loss: 0.3845 - val_acc: 0.8125\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0096 - acc: 0.9967 - val_loss: 0.0056 - val_acc: 1.0000\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0034 - acc: 0.9987 - val_loss: 0.0299 - val_acc: 1.0000\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0029 - acc: 0.9987 - val_loss: 0.0644 - val_acc: 0.9375\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0116 - acc: 0.9965 - val_loss: 0.1713 - val_acc: 0.9375\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0050 - acc: 0.9985 - val_loss: 0.0738 - val_acc: 0.9375\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0087 - acc: 0.9965 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0054 - acc: 0.9977 - val_loss: 0.0280 - val_acc: 1.0000\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0041 - acc: 0.9992 - val_loss: 0.1329 - val_acc: 0.9375\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0061 - acc: 0.9985 - val_loss: 0.5123 - val_acc: 0.8125\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0043 - acc: 0.9985 - val_loss: 1.4290 - val_acc: 0.7500\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0035 - acc: 0.9987 - val_loss: 0.2992 - val_acc: 0.9375\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0056 - acc: 0.9985 - val_loss: 0.3337 - val_acc: 0.8750\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0054 - acc: 0.9985 - val_loss: 0.0134 - val_acc: 1.0000\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0039 - acc: 0.9987 - val_loss: 0.0717 - val_acc: 0.9375\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0041 - acc: 0.9983 - val_loss: 1.8253 - val_acc: 0.6875\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0033 - acc: 0.9988 - val_loss: 0.1257 - val_acc: 0.9375\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0025 - acc: 0.9994 - val_loss: 0.0387 - val_acc: 1.0000\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0021 - acc: 0.9988 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0080 - acc: 0.9971 - val_loss: 0.0126 - val_acc: 1.0000\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0046 - acc: 0.9985 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0054 - acc: 0.9975 - val_loss: 0.2982 - val_acc: 0.8125\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.1265 - val_acc: 0.9375\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0031 - acc: 0.9987 - val_loss: 0.0079 - val_acc: 1.0000\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0058 - acc: 0.9983 - val_loss: 0.0230 - val_acc: 1.0000\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0064 - acc: 0.9987 - val_loss: 0.2025 - val_acc: 0.8750\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0028 - acc: 0.9990 - val_loss: 0.7972 - val_acc: 0.8125\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.0555 - val_acc: 1.0000\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0053 - acc: 0.9985 - val_loss: 0.5496 - val_acc: 0.7500\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0074 - acc: 0.9979 - val_loss: 0.0558 - val_acc: 0.9375\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0046 - acc: 0.9985 - val_loss: 0.0082 - val_acc: 1.0000\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0102 - val_acc: 1.0000\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0042 - acc: 0.9987 - val_loss: 2.6953 - val_acc: 0.6250\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0065 - acc: 0.9977 - val_loss: 0.0057 - val_acc: 1.0000\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0014 - acc: 0.9998 - val_loss: 0.3182 - val_acc: 0.8750\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0035 - acc: 0.9988 - val_loss: 0.0101 - val_acc: 1.0000\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0037 - acc: 0.9988 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0044 - acc: 0.9985 - val_loss: 1.1944 - val_acc: 0.7500\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0063 - acc: 0.9973 - val_loss: 0.0104 - val_acc: 1.0000\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0025 - acc: 0.9992 - val_loss: 0.6931 - val_acc: 0.7500\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.0433 - val_acc: 1.0000\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0047 - acc: 0.9990 - val_loss: 0.0228 - val_acc: 1.0000\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0042 - acc: 0.9985 - val_loss: 3.2725 - val_acc: 0.6875\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0054 - acc: 0.9977 - val_loss: 0.1088 - val_acc: 0.9375\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0034 - acc: 0.9988 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0034 - acc: 0.9988 - val_loss: 0.2245 - val_acc: 0.8750\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.1146 - val_acc: 0.9375\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0049 - acc: 0.9979 - val_loss: 4.3106 - val_acc: 0.6875\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0034 - acc: 0.9988 - val_loss: 0.0098 - val_acc: 1.0000\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0032 - acc: 0.9988 - val_loss: 0.9635 - val_acc: 0.7500\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0058 - acc: 0.9981 - val_loss: 0.1457 - val_acc: 0.9375\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0030 - acc: 0.9994 - val_loss: 0.0496 - val_acc: 1.0000\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.0188 - val_acc: 1.0000\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0045 - acc: 0.9981 - val_loss: 0.9421 - val_acc: 0.6875\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0059 - acc: 0.9983 - val_loss: 0.4984 - val_acc: 0.8125\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0043 - acc: 0.9990 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0068 - acc: 0.9979 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0031 - acc: 0.9988 - val_loss: 0.0212 - val_acc: 1.0000\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 47s 145ms/step - loss: 0.0020 - acc: 0.9996 - val_loss: 0.1715 - val_acc: 0.8750\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0025 - acc: 0.9988 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0042 - acc: 0.9990 - val_loss: 2.4567 - val_acc: 0.6250\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0033 - acc: 0.9992 - val_loss: 0.0077 - val_acc: 1.0000\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 46s 143ms/step - loss: 0.0038 - acc: 0.9992 - val_loss: 0.0116 - val_acc: 1.0000\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0038 - acc: 0.9990 - val_loss: 0.0597 - val_acc: 1.0000\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0040 - acc: 0.9988 - val_loss: 0.8786 - val_acc: 0.7500\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0016 - acc: 0.9996 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0041 - acc: 0.9979 - val_loss: 0.0780 - val_acc: 1.0000\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0018 - acc: 0.9996 - val_loss: 0.1625 - val_acc: 0.8750\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0021 - acc: 0.9992 - val_loss: 6.6412e-04 - val_acc: 1.0000\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0031 - acc: 0.9985 - val_loss: 2.8343e-04 - val_acc: 1.0000\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.0736 - val_acc: 1.0000\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0048 - acc: 0.9985 - val_loss: 3.7022 - val_acc: 0.6250\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0045 - acc: 0.9987 - val_loss: 0.1129 - val_acc: 0.9375\n",
      "Epoch 159/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0063 - acc: 0.9981 - val_loss: 0.0060 - val_acc: 1.0000\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0023 - acc: 0.9990 - val_loss: 0.1374 - val_acc: 0.8750\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 47s 143ms/step - loss: 0.0030 - acc: 0.9992 - val_loss: 0.6777 - val_acc: 0.7500\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0027 - acc: 0.9992 - val_loss: 0.0151 - val_acc: 1.0000\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 48s 147ms/step - loss: 0.0023 - acc: 0.9990 - val_loss: 0.0059 - val_acc: 1.0000\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0042 - acc: 0.9992 - val_loss: 0.2686 - val_acc: 0.9375\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 47s 144ms/step - loss: 0.0033 - acc: 0.9988 - val_loss: 0.0266 - val_acc: 1.0000\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 49s 151ms/step - loss: 0.0014 - acc: 0.9996 - val_loss: 0.0142 - val_acc: 1.0000\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 181s 556ms/step - loss: 0.0039 - acc: 0.9988 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 49s 150ms/step - loss: 0.0036 - acc: 0.9985 - val_loss: 0.2626 - val_acc: 0.8750\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 48s 146ms/step - loss: 0.0028 - acc: 0.9992 - val_loss: 0.5949 - val_acc: 0.8125\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 50s 153ms/step - loss: 0.0017 - acc: 0.9994 - val_loss: 0.0210 - val_acc: 1.0000\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 50s 154ms/step - loss: 0.0026 - acc: 0.9992 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 49s 149ms/step - loss: 0.0031 - acc: 0.9992 - val_loss: 0.1394 - val_acc: 0.9375\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 50s 153ms/step - loss: 7.7698e-04 - acc: 0.9998 - val_loss: 0.1469 - val_acc: 0.8750\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 48s 146ms/step - loss: 0.0017 - acc: 0.9996 - val_loss: 0.0105 - val_acc: 1.0000\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 48s 147ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.3019 - val_acc: 0.8125\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 48s 147ms/step - loss: 0.0030 - acc: 0.9987 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 48s 147ms/step - loss: 0.0024 - acc: 0.9994 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 48s 147ms/step - loss: 0.0033 - acc: 0.9994 - val_loss: 0.0167 - val_acc: 1.0000\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 51s 155ms/step - loss: 0.0059 - acc: 0.9987 - val_loss: 0.0157 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 50s 153ms/step - loss: 0.0017 - acc: 0.9992 - val_loss: 0.6016 - val_acc: 0.8125\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 48s 148ms/step - loss: 0.0020 - acc: 0.9994 - val_loss: 1.6947 - val_acc: 0.6875\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 49s 151ms/step - loss: 0.0017 - acc: 0.9994 - val_loss: 0.8950 - val_acc: 0.8125\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 48s 148ms/step - loss: 0.0058 - acc: 0.9979 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 50s 152ms/step - loss: 0.0013 - acc: 0.9998 - val_loss: 0.0840 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 51s 155ms/step - loss: 0.0038 - acc: 0.9987 - val_loss: 0.0051 - val_acc: 1.0000\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 50s 154ms/step - loss: 0.0027 - acc: 0.9992 - val_loss: 0.8555 - val_acc: 0.7500\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 50s 153ms/step - loss: 0.0018 - acc: 0.9994 - val_loss: 0.0617 - val_acc: 0.9375\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 54s 165ms/step - loss: 0.0036 - acc: 0.9981 - val_loss: 0.2033 - val_acc: 0.9375\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 51s 156ms/step - loss: 0.0022 - acc: 0.9990 - val_loss: 0.3674 - val_acc: 0.8750\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 50s 155ms/step - loss: 0.0022 - acc: 0.9992 - val_loss: 0.8863 - val_acc: 0.7500\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 52s 159ms/step - loss: 0.0039 - acc: 0.9987 - val_loss: 0.0905 - val_acc: 0.9375\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 51s 155ms/step - loss: 0.0011 - acc: 0.9996 - val_loss: 0.5708 - val_acc: 0.8750\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 51s 155ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 1.4802 - val_acc: 0.7500\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 52s 160ms/step - loss: 0.0031 - acc: 0.9988 - val_loss: 0.1146 - val_acc: 0.9375\n",
      "Epoch 195/200\n",
      "326/326 [==============================] - 51s 156ms/step - loss: 0.0025 - acc: 0.9992 - val_loss: 0.4566 - val_acc: 0.8750\n",
      "Epoch 196/200\n",
      "326/326 [==============================] - 52s 161ms/step - loss: 0.0042 - acc: 0.9987 - val_loss: 0.2833 - val_acc: 0.9375\n",
      "Epoch 197/200\n",
      "326/326 [==============================] - 50s 154ms/step - loss: 0.0032 - acc: 0.9988 - val_loss: 0.1686 - val_acc: 0.9375\n",
      "Epoch 198/200\n",
      "326/326 [==============================] - 51s 157ms/step - loss: 0.0039 - acc: 0.9987 - val_loss: 0.1362 - val_acc: 0.8750\n",
      "Epoch 199/200\n",
      "326/326 [==============================] - 50s 154ms/step - loss: 0.0021 - acc: 0.9990 - val_loss: 0.0254 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "326/326 [==============================] - 50s 153ms/step - loss: 0.0026 - acc: 0.9990 - val_loss: 1.1380 - val_acc: 0.8125\n"
     ]
    }
   ],
   "source": [
    "with K.tf.device('/gpu:0'):\n",
    "    model.compile(Adam(lr=0.0001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    model.fit_generator(train_batches,validation_data=valid_batches,epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 9s 114ms/step\n"
     ]
    }
   ],
   "source": [
    "p = model.predict_generator(test_batches, verbose=True)\n",
    "pre = pd.DataFrame(p)\n",
    "pre[\"filename\"] = test_batches.filenames\n",
    "pre[\"label\"] = (pre[\"filename\"].str.contains(\"PNEUMONIA\")).apply(int)\n",
    "pre['pre'] = (pre[1]>0.5).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9948717948717949"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(pre[\"label\"],pre[\"pre\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8342373438527284"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(pre[\"label\"],pre[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2289c249550>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2289c0b3c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tpr,fpr,thres = roc_curve(pre[\"label\"],pre[1])\n",
    "roc = pd.DataFrame([tpr,fpr]).T\n",
    "roc.plot(x=0,y=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
